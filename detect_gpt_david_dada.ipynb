{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JpnpE4oz_7iGaRYXanXdhrAEOnXJEuD_",
      "authorship_tag": "ABX9TyM2QHz3YlBNQvRSLPlmm3GY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dada-Tech/detect-gpt/blob/master/detect_gpt_david_dada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "import os"
      ],
      "metadata": {
        "id": "jR32WRviVjcT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PAF3NCrG0b7"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Dada-Tech/detect-gpt.git\n",
        "!git pull\n",
        "\n",
        "# https://github.com/Dada-Tech/detect-gpt.git\n",
        "# https://github.com/eric-mitchell/detect-gpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## directories\n",
        "directory = 'detect-gpt'\n",
        "base_dir = '/content/code'\n",
        "\n",
        "if os.getcwd() != base_dir + directory:\n",
        "  os.chdir('detect-gpt')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "## drive mounts\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eJR3ywcDVkXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29197b68-af64-4c8c-edf4-cd63c14dc11f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/code/detect-gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## dir deletion\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/detect-gpt', ignore_errors=True)"
      ],
      "metadata": {
        "id": "S3BtQlZCupxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## requirements\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "wO6pzxhVXGs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU enabled\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-jib2hBSp7tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## script run (main.sh)\n",
        "\n",
        "!./paper_scripts/main.sh"
      ],
      "metadata": {
        "id": "Yq6I7NV_WuZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## manual run\n",
        "\n",
        "!python run.py --batch_size 5 --n_samples 100 --n_perturbation_list 1,3 --base_model_name gpt2 --mask_filling_model_name t5-small --dataset reviews --cache_dir code\n",
        "# !python run.py --batch_size 5 --n_samples 100 --n_perturbation_list 1,3 --dataset reviews --cache_dir code\n"
      ],
      "metadata": {
        "id": "Nzoj4_ZNXsrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a30ba72-62ce-4f62-a0b0-e028f54a152c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to absolute path: /content/code/detect-gpt/tmp_results/gpt2-t5-small-temp/2023-03-31-03-25-22-788550-fp32-0.3-1-reviews-100\n",
            "Using cache dir code\n",
            "Loading BASE model gpt2...\n",
            "Loading mask filling model t5-small...\n",
            "MOVING BASE MODEL TO GPU...DONE (2.07s)\n",
            "Loading dataset reviews...\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
            "Total number of samples: 497\n",
            "Average number of words: 66.01207243460765\n",
            "Generating samples for batch 0 of 20\n",
            "2023-03-31 03:25:42.840012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Generating samples for batch 1 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 2 of 20\n",
            "Generating samples for batch 3 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 4 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 5 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 6 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 7 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 8 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 9 of 20\n",
            "Generating samples for batch 10 of 20\n",
            "Generating samples for batch 11 of 20\n",
            "Generating samples for batch 12 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 13 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 14 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 15 of 20\n",
            "Generating samples for batch 16 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 17 of 20\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 18 of 20\n",
            "Generating samples for batch 19 of 20\n",
            "Writing raw data to tmp_results/gpt2-t5-small-temp/2023-03-31-03-25-22-788550-fp32-0.3-1-reviews-100/raw_data.json\n",
            "Computing likelihood criterion: 100% 20/20 [00:03<00:00,  6.44it/s]\n",
            "likelihood_threshold ROC AUC: 0.6870499999999999, PR AUC: 0.7444835808616959\n",
            "Computing rank criterion: 100% 20/20 [00:04<00:00,  4.76it/s]\n",
            "rank_threshold ROC AUC: 0.65505, PR AUC: 0.7156922708946549\n",
            "Computing log_rank criterion: 100% 20/20 [00:03<00:00,  5.83it/s]\n",
            "log_rank_threshold ROC AUC: 0.70435, PR AUC: 0.764794530436802\n",
            "Computing entropy criterion: 100% 20/20 [00:02<00:00,  7.19it/s]\n",
            "entropy_threshold ROC AUC: 0.4729, PR AUC: 0.4819354518818154\n",
            "Beginning supervised evaluation with roberta-base-openai-detector...\n",
            "Downloading (â€¦)lve/main/config.json: 100% 624/624 [00:00<00:00, 102kB/s]\n",
            "Downloading pytorch_model.bin: 100% 501M/501M [00:06<00:00, 77.8MB/s]\n",
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading (â€¦)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 2.88MB/s]\n",
            "Downloading (â€¦)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 1.84MB/s]\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.31MB/s]\n",
            "Evaluating real: 100% 20/20 [00:00<00:00, 22.92it/s]\n",
            "Evaluating fake: 100% 20/20 [00:00<00:00, 24.87it/s]\n",
            "roberta-base-openai-detector ROC AUC: 0.82475, PR AUC: 0.8361085811754221\n",
            "Beginning supervised evaluation with roberta-large-openai-detector...\n",
            "Downloading (â€¦)lve/main/config.json: 100% 519/519 [00:00<00:00, 189kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.43G/1.43G [00:40<00:00, 35.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading (â€¦)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 2.89MB/s]\n",
            "Downloading (â€¦)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 1.80MB/s]\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.30MB/s]\n",
            "Evaluating real: 100% 20/20 [00:02<00:00,  7.20it/s]\n",
            "Evaluating fake: 100% 20/20 [00:02<00:00,  7.58it/s]\n",
            "roberta-large-openai-detector ROC AUC: 0.87485, PR AUC: 0.894699482114096\n",
            "MOVING MASK MODEL TO GPU...DONE (0.19s)\n",
            "Applying perturbations: 100% 5/5 [00:07<00:00,  1.51s/it]\n",
            "Applying perturbations:   0% 0/5 [00:00<?, ?it/s]WARNING: 1 texts have no fills. Trying again [attempt 1].\n",
            "WARNING: 1 texts have no fills. Trying again [attempt 2].\n",
            "Applying perturbations: 100% 5/5 [00:08<00:00,  1.73s/it]\n",
            "MOVING BASE MODEL TO GPU...DONE (0.20s)\n",
            "Computing log likelihoods: 100% 100/100 [00:06<00:00, 15.22it/s]\n",
            "perturbation_1_d ROC AUC: 0.6061, PR AUC: 0.5806143909497781\n",
            "perturbation_1_z ROC AUC: 0.6061, PR AUC: 0.5806143909497781\n",
            "MOVING MASK MODEL TO GPU...DONE (0.18s)\n",
            "Applying perturbations: 100% 15/15 [00:16<00:00,  1.11s/it]\n",
            "Applying perturbations:  53% 8/15 [00:12<00:08,  1.28s/it]WARNING: 1 texts have no fills. Trying again [attempt 1].\n",
            "Applying perturbations: 100% 15/15 [00:22<00:00,  1.47s/it]\n",
            "MOVING BASE MODEL TO GPU...DONE (0.20s)\n",
            "Computing log likelihoods: 100% 100/100 [00:12<00:00,  8.23it/s]\n",
            "perturbation_3_d ROC AUC: 0.5879, PR AUC: 0.583996737252357\n",
            "perturbation_3_z ROC AUC: 0.5708, PR AUC: 0.5661005079013796\n",
            "perturbation_1_d roc_auc: 0.606\n",
            "perturbation_1_z roc_auc: 0.606\n",
            "perturbation_3_d roc_auc: 0.588\n",
            "perturbation_3_z roc_auc: 0.571\n",
            "likelihood_threshold roc_auc: 0.687\n",
            "rank_threshold roc_auc: 0.655\n",
            "log_rank_threshold roc_auc: 0.704\n",
            "entropy_threshold roc_auc: 0.473\n",
            "roberta-base-openai-detector roc_auc: 0.825\n",
            "roberta-large-openai-detector roc_auc: 0.875\n",
            "Used an *estimated* 0 API tokens (may be inaccurate)\n"
          ]
        }
      ]
    }
  ]
}